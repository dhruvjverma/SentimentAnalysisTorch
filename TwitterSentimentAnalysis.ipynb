{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20fee7f-5dc7-43e8-9aab-e72d158d31ca",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using RNN\n",
    "### By: Dhruv Jayant Verma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a50b9d-230a-4335-a68b-387d2d2f4e86",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9affaa-e484-479e-96ab-d15c03767fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target         ids                          date      flag  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names as they are not included in the CSV file\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('twitter.csv',\n",
    "                 encoding='ISO-8859-1', names=column_names) # avoids errors in reading fie\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c083d0e-5824-4002-8ff0-3ceeee59cb0d",
   "metadata": {},
   "source": [
    "#### Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccfdfde-15c0-426a-94c0-1a416bf01c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the target variable\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab5d18-8cf9-49f0-b1b1-c2fb8e7c1c85",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "#### Mapping Labels\n",
    "I'll map the labels from `[0, 4]` to `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b261ae2a-dfd3-4f44-9b31-0776443afa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map target labels to 0 and 1\n",
    "df['target'] = df['target'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10576e6c-9155-46a0-8877-d93fb9f06a1d",
   "metadata": {},
   "source": [
    "## Using a Sample of Original Data\n",
    "Since original dataset is too big to compute, I'll use `10%` of the original data for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2cadb3-c679-4f69-ba87-69f3b9cefbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a 10% sample of the data for faster processing|\n",
    "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a4ffd-0ba1-4a8c-bac6-8de0e10044e8",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "I'll preprocess the text data to remove noise and prepare it for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c516d016-bdb5-4834-9621-6de29bdb20d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove special characters, numbers, punctuations\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and stem the tokens\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6122f6c-e0bc-4c7d-860e-4156d09af294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "      <td>ahhh hope ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "      <td>cool tweet app razr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "      <td>know famili drama lamehey next time u hang kim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "      <td>school email wont open geographi stuff revis s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>upper airway problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "1       0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "2       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "3       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "4       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
       "1      sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...   \n",
       "2       sammydearr  @TiannaChaos i know  just family drama. its la...   \n",
       "3      Lamb_Leanne  School email won't open  and I have geography ...   \n",
       "4      yogicerdito                             upper airways problem    \n",
       "\n",
       "                                          clean_text  \n",
       "0                                       ahhh hope ok  \n",
       "1                                cool tweet app razr  \n",
       "2  know famili drama lamehey next time u hang kim...  \n",
       "3  school email wont open geographi stuff revis s...  \n",
       "4                               upper airway problem  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d29ff2-8192-4961-ae1f-bc9f44119084",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building\n",
    "### Creating the Vocabulary\n",
    "Here, I'll create a vocabulary dictionary by extracting all the unique words from the cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eeac3cf-246b-4bf5-9e6a-938e7e459fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all texts\n",
    "all_words = ' '.join(df['clean_text']).split()\n",
    "\n",
    "# Compute word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Create a vocabulary dictionary mapping words to indices\n",
    "vocab = {word: idx + 1 for idx, (word, count) in enumerate(word_counts.items())}\n",
    "\n",
    "# Add a special token for unknown words\n",
    "vocab_size = len(vocab) + 1  # Plus one for unknown token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8427c-7a8c-480d-aa50-72aef46a796e",
   "metadata": {},
   "source": [
    "## Encoding and Padding\n",
    "I'll encode each tweet as a sequence of integers and pad them to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "862c02f2-1f08-416e-8dcb-ffd0674c13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def encode_tweet(text):\n",
    "    return [vocab.get(word, 0) for word in text.split()]\n",
    "\n",
    "df['encoded_text'] = df['clean_text'].apply(encode_tweet)\n",
    "\n",
    "# Determine the maximum length (you can set a fixed length or compute the max)\n",
    "max_len = 50\n",
    "\n",
    "def pad_sequence_custom(sequence):\n",
    "    if len(sequence) < max_len:\n",
    "        return sequence + [0] * (max_len - len(sequence))\n",
    "    else:\n",
    "        return sequence[:max_len]\n",
    "\n",
    "df['padded_text'] = df['encoded_text'].apply(pad_sequence_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee51ad9-fc29-441d-8977-48276887e65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>encoded_text</th>\n",
       "      <th>padded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2200003196</td>\n",
       "      <td>Tue Jun 16 18:18:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaLaLindsey0609</td>\n",
       "      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n",
       "      <td>ahhh hope ok</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467998485</td>\n",
       "      <td>Mon Apr 06 23:11:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sexygrneyes</td>\n",
       "      <td>@misstoriblack cool , i have no tweet apps  fo...</td>\n",
       "      <td>cool tweet app razr</td>\n",
       "      <td>[4, 5, 6, 7]</td>\n",
       "      <td>[4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2300048954</td>\n",
       "      <td>Tue Jun 23 13:40:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammydearr</td>\n",
       "      <td>@TiannaChaos i know  just family drama. its la...</td>\n",
       "      <td>know famili drama lamehey next time u hang kim...</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 14, 18,...</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 14, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1993474027</td>\n",
       "      <td>Mon Jun 01 10:26:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Lamb_Leanne</td>\n",
       "      <td>School email won't open  and I have geography ...</td>\n",
       "      <td>school email wont open geographi stuff revis s...</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 24]</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 31, 24, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2256550904</td>\n",
       "      <td>Sat Jun 20 12:56:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yogicerdito</td>\n",
       "      <td>upper airways problem</td>\n",
       "      <td>upper airway problem</td>\n",
       "      <td>[32, 33, 34]</td>\n",
       "      <td>[32, 33, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  2200003196  Tue Jun 16 18:18:12 PDT 2009  NO_QUERY   \n",
       "1       0  1467998485  Mon Apr 06 23:11:14 PDT 2009  NO_QUERY   \n",
       "2       0  2300048954  Tue Jun 23 13:40:11 PDT 2009  NO_QUERY   \n",
       "3       0  1993474027  Mon Jun 01 10:26:07 PDT 2009  NO_QUERY   \n",
       "4       0  2256550904  Sat Jun 20 12:56:51 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  LaLaLindsey0609             @chrishasboobs AHHH I HOPE YOUR OK!!!    \n",
       "1      sexygrneyes  @misstoriblack cool , i have no tweet apps  fo...   \n",
       "2       sammydearr  @TiannaChaos i know  just family drama. its la...   \n",
       "3      Lamb_Leanne  School email won't open  and I have geography ...   \n",
       "4      yogicerdito                             upper airways problem    \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                       ahhh hope ok   \n",
       "1                                cool tweet app razr   \n",
       "2  know famili drama lamehey next time u hang kim...   \n",
       "3  school email wont open geographi stuff revis s...   \n",
       "4                               upper airway problem   \n",
       "\n",
       "                                        encoded_text  \\\n",
       "0                                          [1, 2, 3]   \n",
       "1                                       [4, 5, 6, 7]   \n",
       "2  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 14, 18,...   \n",
       "3               [24, 25, 26, 27, 28, 29, 30, 31, 24]   \n",
       "4                                       [32, 33, 34]   \n",
       "\n",
       "                                         padded_text  \n",
       "0  [1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 14, 18,...  \n",
       "3  [24, 25, 26, 27, 28, 29, 30, 31, 24, 0, 0, 0, ...  \n",
       "4  [32, 33, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904ebdb-58a6-4de1-a820-4a2f01a5aa58",
   "metadata": {},
   "source": [
    "## Creating PyTorch Datasets and DataLoaders\n",
    "### Creating the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a52be0dd-3d36-414f-a446-95ea235530b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Sentiment(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a757a5-a506-4c96-ad96-66fc0dbd2659",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "Split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1bd387a-600c-4db7-84eb-e261ed865459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = df['padded_text'].tolist()\n",
    "y = df['target'].tolist()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Sentiment(X_train, y_train)\n",
    "val_dataset = Sentiment(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4b7a-9365-49cc-9fd0-7219d480a514",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d2efd1-f107-4d3b-ab9a-534ac87bb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea342a-7af7-4552-b530-095822f925ca",
   "metadata": {},
   "source": [
    "## Making the Neural Network Architecture\n",
    "I'll use a RNN model architecture, utilizing an Embedding layer followed by an LSTM or GRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87ee81c-27bd-4766-a5c9-f4ad8a848753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers=2, bidirectional=True, dropout=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenate the final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de020e4-9967-4a2f-8958-8906eb2f1cf4",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e298cf8-013d-4efd-92da-295c2f26f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size  # As calculated before\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Binary classification\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "\n",
    "model = SentimentRNN(vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                     n_layers, bidirectional, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bfe369-ed62-48ce-8551-4d5f356e9e20",
   "metadata": {},
   "source": [
    "## Moving Model to Gpu for Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11c5f421-e0bf-4658-9e7f-ab4f43d0e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    print('CUDA not available')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932942f4-84d8-4451-b84c-37ed2293af8b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### Defining Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a1894ed-ecc2-4afb-bfd1-ec2e7db2a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bcd14-5f76-4ddc-87ee-ef00860dc41b",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d151382-6f1a-4c3f-b0c2-241840b82ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.545\n",
      "Validation Loss: 0.482\n",
      "Epoch 2\n",
      "Training Loss: 0.478\n",
      "Validation Loss: 0.469\n"
     ]
    }
   ],
   "source": [
    "epochs = 2  # Increase the number of epochs as needed\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Training Loss: {epoch_loss / len(train_loader):.3f}')\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e715beb-f459-4a95-8213-2663ee14db3b",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "### Calculating Metrics\n",
    "I'll calculate accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de40d948-4e03-4842-9654-596458216284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776\n",
      "Precision: 0.762\n",
      "Recall: 0.805\n",
      "F1 Score: 0.782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Lists to store all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for texts, labels in val_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        preds = torch.round(torch.sigmoid(predictions))\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.3f}')\n",
    "print(f'Precision: {precision:.3f}')\n",
    "print(f'Recall: {recall:.3f}')\n",
    "print(f'F1 Score: {f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352b753-8c20-44ed-a0c7-bb8b58bc679f",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Define a function to preprocess and predict the sentiment of new tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a38d00e-d0a9-4da5-9a2b-96c9e6a83fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (Score: 0.981)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    model.eval()\n",
    "    text = preprocess_text(text)\n",
    "    encoded = [vocab.get(word, 0) for word in text.split()]\n",
    "    padded = pad_sequence_custom(encoded)\n",
    "    tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()\n",
    "\n",
    "# Example usage\n",
    "new_tweet = \"I absolutely love the new design of your website!\"\n",
    "score = predict_sentiment(model, new_tweet)\n",
    "sentiment = 'Positive' if score >= 0.5 else 'Negative'\n",
    "print(f'Sentiment: {sentiment} (Score: {score:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f1749-85fb-43bb-9936-578a2b43a7e5",
   "metadata": {},
   "source": [
    "## Saving the Model for future use and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3050e5-d184-4d9a-a849-5beb983a4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# save model to a file\n",
    "joblib.dump(model, 'sentiment_analysis.joblib')\n",
    "\n",
    "# load the model\n",
    "loaded_model = joblib.load('sentiment_analysis.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a528c2e6-e2be-458c-9522-3d7f60188eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative (Score: 0.018)\n"
     ]
    }
   ],
   "source": [
    "example = \"She is a bad person. I really hate her!\"\n",
    "score = predict_sentiment(model, example)\n",
    "sentiment = 'Positive' if score >= 0.5 else 'Negative'\n",
    "print(f'Sentiment: {sentiment} (Score: {score:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa24626-61cd-493a-a084-924941255781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
